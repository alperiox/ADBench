{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alperiox/ADBench/blob/main/nb/Oute_TTS_(1B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CADH_fo_U0SB"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NPPXizTU0SD"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCDFTFmBU0SD"
      },
      "source": [
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJKbAJ9pU0SE"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2JxnTqWNU0SE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install omegaconf einx\n",
        "!rm -rf OuteTTS && git clone https://github.com/edwko/OuteTTS\n",
        "import os\n",
        "os.remove(\"/content/OuteTTS/outetts/models/gguf_model.py\")\n",
        "os.remove(\"/content/OuteTTS/outetts/interface.py\")\n",
        "os.remove(\"/content/OuteTTS/outetts/__init__.py\")\n",
        "!pip install pyloudnorm openai-whisper uroman MeCab loguru flatten_dict ffmpy randomname argbind tiktoken ftfy\n",
        "!pip install descript-audio-codec descript-audiotools julius openai-whisper --no-deps\n",
        "%env UNSLOTH_DISABLE_FAST_GENERATION = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkWYsztAs9Ky"
      },
      "source": [
        "### Unsloth\n",
        "\n",
        "`FastModel` supports loading nearly any model now! This includes Vision and Text models!\n",
        "\n",
        "Thank you to [Etherl](https://huggingface.co/Etherll) for creating this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "e19818d7-66fa-4f01-ff66-585420312f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.7.8: Fast Llama patching. Transformers: 4.53.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any for long context!\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-OuteTTS-1.0-1B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, # Set to None for auto detection\n",
        "    load_in_4bit = False, # Set to True for 4bit which reduces memory\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "c53b06e9-95f3-43f7-fd19-01eea30fddaf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3973338733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = FastModel.get_peft_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget_modules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"q_proj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v_proj\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlora_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/vision.py\u001b[0m in \u001b[0;36mget_peft_model\u001b[0;34m(model, r, target_modules, lora_alpha, lora_dropout, bias, finetune_vision_layers, finetune_language_layers, finetune_attention_modules, finetune_mlp_modules, layers_to_transform, layers_pattern, use_gradient_checkpointing, random_state, max_seq_length, use_rslora, modules_to_save, init_lora_weights, loftq_config, task_type, temporary_location, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mloftq_config\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mloftq_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         )\n\u001b[0;32m--> 636\u001b[0;31m         model = prepare_model_for_kbit_training(\n\u001b[0m\u001b[1;32m    637\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0muse_gradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/_utils.py\u001b[0m in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing, use_reentrant)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0muse_reentrant\u001b[0m              \u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m ) -> Any:\n\u001b[0;32m--> 649\u001b[0;31m     return prepare_model_for_training(\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mmodel\u001b[0m                      \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0muse_gradient_checkpointing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_gradient_checkpointing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/training_utils.py\u001b[0m in \u001b[0;36mprepare_model_for_training\u001b[0;34m(model, use_gradient_checkpointing, use_reentrant, full_finetuning, train_layernorms, train_embedding, train_lm_head, float32_mixed_precision)\u001b[0m\n\u001b[1;32m     99\u001b[0m ) -> Any:\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# All Unsloth Zoo code licensed under LGPLv3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gradient_checkpointing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unsloth\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_reentrant\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_finetuning\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"v_proj\",],\n",
        "    lora_alpha = 128,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"False\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep  \n",
        "\n",
        "We will use the `MrDragonFox/Elise`, which is designed for training TTS models. Ensure that your dataset follows the required format: **text, audio**, but maintaining the correct structure is essential for optimal training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the max audio length at raw_Ds\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_ds1 = load_dataset(\"omersaidd/tts_murat_eken\", split=\"train\")\n",
        "\n",
        "\n",
        "max_audio_length_samples = 0\n",
        "sampling_rate = None\n",
        "lengths1 = []\n",
        "for example in raw_ds1:\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    audio_length_samples = len(audio_array)\n",
        "    if audio_length_samples > max_audio_length_samples:\n",
        "        max_audio_length_samples = audio_length_samples\n",
        "        sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "    lengths1.append(audio_length_samples / sampling_rate)\n",
        "\n",
        "max_audio_length_seconds = max_audio_length_samples / sampling_rate if sampling_rate else 0\n",
        "\n",
        "print(f\"Maximum audio length in samples: {max_audio_length_samples}\")\n",
        "print(f\"Sampling rate: {sampling_rate}\")\n",
        "print(f\"Maximum audio length in seconds: {max_audio_length_seconds}\")"
      ],
      "metadata": {
        "id": "H9mlIBxQzp3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_long_audio(example):\n",
        "    audio_array = example[\"audio\"][\"array\"]\n",
        "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
        "    audio_length_seconds = len(audio_array) / sampling_rate\n",
        "    return audio_length_seconds <= 10\n",
        "\n",
        "raw_ds1 = raw_ds1.filter(filter_long_audio)\n",
        "display(raw_ds1)"
      ],
      "metadata": {
        "id": "xFzArjqsYyYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(lengths1, bins=50)\n",
        "plt.xlabel('Audio Length (seconds)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Audio Lengths')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FYZ1dPrxYSv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio"
      ],
      "metadata": {
        "id": "SNyMb-GVoVO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ds1 = raw_ds1.cast_column(\"audio\", Audio(sampling_rate=22050))\n",
        "display(raw_ds1)"
      ],
      "metadata": {
        "id": "8YXph0MgXgTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset,Audio,Dataset\n",
        "from copy import deepcopy\n",
        "dataset = deepcopy(raw_ds1)\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=24000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK94B-Pfioto"
      },
      "outputs": [],
      "source": [
        "#@title Tokenization Function\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "import tempfile\n",
        "from datasets import Dataset\n",
        "import sys\n",
        "sys.path.append('OuteTTS')\n",
        "import os\n",
        "import dac\n",
        "# V3 Imports\n",
        "from outetts.version.v3.audio_processor import AudioProcessor\n",
        "from outetts.version.v3.prompt_processor import PromptProcessor\n",
        "from outetts.dac.interface import DacInterface\n",
        "from outetts.models.config import ModelConfig # Need a dummy config for AudioProcessor\n",
        "import whisper\n",
        "from outetts.utils.preprocessing import text_normalizations\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "\n",
        "class DataCreationV3:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model_tokenizer_path: str,\n",
        "            whisper_model_name: str = \"turbo\",\n",
        "            device: str = None\n",
        "        ):\n",
        "\n",
        "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Create a dummy ModelConfig mainly for device and paths needed by AudioProcessor/DacInterface\n",
        "        dummy_config = ModelConfig(\n",
        "            tokenizer_path=model_tokenizer_path,\n",
        "            device=self.device,\n",
        "            audio_codec_path=None # Let AudioProcessor use default DAC path\n",
        "        )\n",
        "        self.audio_processor = AudioProcessor(config=dummy_config)\n",
        "        self.prompt_processor = PromptProcessor(model_tokenizer_path)\n",
        "\n",
        "        print(f\"Loading Whisper model: {whisper_model_name} on {self.device}\")\n",
        "        self.whisper_model = whisper.load_model(whisper_model_name, device=self.device)\n",
        "        print(\"Whisper model loaded.\")\n",
        "\n",
        "    # Renamed and adapted from the previous version\n",
        "    def create_speaker_representation(self, audio_bytes: bytes, transcript: str):\n",
        "        \"\"\"\n",
        "        Creates a v3-compatible speaker dictionary using Whisper and AudioProcessor.\n",
        "        \"\"\"\n",
        "        if not audio_bytes or not transcript:\n",
        "             print(\"Missing audio bytes or transcript in create_speaker_representation.\")\n",
        "             return None\n",
        "\n",
        "        # Whisper needs a file path, so save bytes to a temporary file\n",
        "        try:\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as tmp_audio_file:\n",
        "                tmp_audio_file.write(audio_bytes)\n",
        "                tmp_audio_file.flush() # Ensure data is written\n",
        "\n",
        "                # 1. Get word timings using Whisper\n",
        "                whisper_result = self.whisper_model.transcribe(tmp_audio_file.name, word_timestamps=True)\n",
        "                # Use the provided transcript for consistency, but Whisper timings\n",
        "                normalized_transcript = text_normalizations(transcript)\n",
        "\n",
        "                words_with_timings = []\n",
        "                if whisper_result and 'segments' in whisper_result:\n",
        "                    for segment in whisper_result['segments']:\n",
        "                        if 'words' in segment:\n",
        "                            for word_info in segment['words']:\n",
        "                                # Use original word casing/punctuation from Whisper's output if needed,\n",
        "                                # but strip excess whitespace for consistency.\n",
        "                                cleaned_word = word_info['word'].strip()\n",
        "                                if cleaned_word: # Ignore empty strings\n",
        "                                    words_with_timings.append({\n",
        "                                        'word': cleaned_word,\n",
        "                                        'start': float(word_info['start']),\n",
        "                                        'end': float(word_info['end'])\n",
        "                                    })\n",
        "                else:\n",
        "                    print(f\"Whisper did not return segments/words for: {transcript[:50]}...\")\n",
        "                    return None # Indicate failure\n",
        "\n",
        "                if not words_with_timings:\n",
        "                    print(f\"No word timings extracted by Whisper for: {transcript[:50]}...\")\n",
        "                    return None\n",
        "\n",
        "                # Prepare data dict for AudioProcessor\n",
        "                speaker_data_dict = {\n",
        "                    \"audio\": {\"bytes\": audio_bytes},\n",
        "                    \"text\": normalized_transcript, # Use the potentially normalized transcript\n",
        "                    \"words\": words_with_timings\n",
        "                }\n",
        "\n",
        "                # 2. Use AudioProcessor to create the speaker representation\n",
        "                v3_speaker = self.audio_processor.create_speaker_from_dict(speaker_data_dict)\n",
        "                return v3_speaker\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during speaker creation (Whisper/AudioProcessor): {e}\")\n",
        "            return None # Indicate failure\n",
        "\n",
        "\n",
        "    # --- V3 Changes: run method is now a generator ---\n",
        "    def process_dataset(self, dataset: Dataset):\n",
        "        \"\"\"\n",
        "        Processes a Hugging Face Dataset object in memory and yields training prompts.\n",
        "\n",
        "        Args:\n",
        "            dataset (Dataset): The Hugging Face dataset to process.\n",
        "                               Expected columns: 'text' (str) and 'audio' (dict with 'bytes').\n",
        "\n",
        "        Yields:\n",
        "            str: The processed training prompt string for each valid row.\n",
        "        \"\"\"\n",
        "        processed_count = 0\n",
        "        skipped_count = 0\n",
        "\n",
        "        # Iterate directly over the dataset\n",
        "        for i, item in enumerate(tqdm(dataset, desc=\"Processing Dataset\")):\n",
        "            try:\n",
        "                # --- Adapt to your dataset's column names ---\n",
        "                transcript = item.get('text')\n",
        "                audio_info = item.get('audio')\n",
        "                # --- End Adapt ---\n",
        "\n",
        "                if not transcript or not isinstance(transcript, str):\n",
        "                    print(f\"Row {i}: Skipping due to missing or invalid 'text' column.\")\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                audio_array = audio_info['array']\n",
        "                buffer = io.BytesIO()\n",
        "                # Ensure array is float32 for common compatibility, adjust subtype if needed\n",
        "                sf.write(buffer, audio_array.astype(np.float32), audio_info['sampling_rate'], format='WAV', subtype='FLOAT')\n",
        "                buffer.seek(0)\n",
        "                audio_bytes = buffer.getvalue()\n",
        "\n",
        "                # Create speaker representation\n",
        "                speaker = self.create_speaker_representation(audio_bytes, transcript)\n",
        "\n",
        "                if speaker is None:\n",
        "                    print(f\"Row {i}: Failed to create speaker representation for text: {transcript[:50]}... Skipping.\")\n",
        "                    skipped_count += 1\n",
        "                    continue\n",
        "\n",
        "                # Get the V3 training prompt\n",
        "                prompt = self.prompt_processor.get_training_prompt(speaker)\n",
        "\n",
        "                processed_count += 1\n",
        "                yield prompt # Yield the processed prompt string\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                 print(\"Processing interrupted by user.\")\n",
        "                 break\n",
        "            except Exception as e:\n",
        "                print(f\"Row {i}: Unhandled error processing item: {e}\", exc_info=True)\n",
        "                skipped_count += 1\n",
        "                # Decide if you want to stop on errors or just skip\n",
        "                continue\n",
        "\n",
        "        print(f\"Dataset processing finished. Processed: {processed_count}, Skipped: {skipped_count}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    _MODEL_TOKENIZER_PATH = \"OuteAI/Llama-OuteTTS-1.0-1B\"\n",
        "    _WHISPER_MODEL = \"turbo\" # Or \"small.en\", \"medium.en\", \"large-v2\", etc.\n",
        "\n",
        "\n",
        "    data_processor = DataCreationV3(\n",
        "        model_tokenizer_path=_MODEL_TOKENIZER_PATH,\n",
        "        whisper_model_name=_WHISPER_MODEL\n",
        "    )\n",
        "\n",
        "    # Process the dataset and collect prompts (or process iteratively)\n",
        "    all_prompts = []\n",
        "    print(\"Starting dataset processing...\")\n",
        "    procced_dataset = data_processor.process_dataset(dataset)\n",
        "    for prompt in procced_dataset:\n",
        "        if prompt:\n",
        "             all_prompts.append({'text': prompt})\n",
        "    dataset = Dataset.from_list(all_prompts)\n",
        "    print(\"Moving Whisper model to CPU\")\n",
        "    data_processor.whisper_model.to('cpu')\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUdB40Ep6Ki"
      },
      "outputs": [],
      "source": [
        "input_text = \"Selamlar, ben Freya'yƒ±m!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krYI8PrRJ6MX"
      },
      "outputs": [],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "import torchaudio.transforms as T\n",
        "from transformers import LogitsProcessor\n",
        "import transformers.generation.utils as generation_utils\n",
        "from transformers import AutoModelForCausalLM\n",
        "import re\n",
        "\n",
        "def get_audio(tokens):\n",
        "        decoded_output = tokenizer.batch_decode(tokens, skip_special_tokens=False, past_key_value=None)[0]\n",
        "        c1 = list(map(int,re.findall(r\"<\\|c1_(\\d+)\\|>\", decoded_output)))\n",
        "        c2 = list(map(int,re.findall(r\"<\\|c2_(\\d+)\\|>\", decoded_output)))\n",
        "\n",
        "        t = min(len(c1), len(c2))\n",
        "        c1 = c1[:t]\n",
        "        c2 = c2[:t]\n",
        "        output = [c1,c2]\n",
        "        if not output:\n",
        "            print(\"No audio tokens found in the output\")\n",
        "            return None\n",
        "\n",
        "        return data_processor.audio_processor.audio_codec.decode(\n",
        "            torch.tensor([output], dtype=torch.int64).to(data_processor.audio_processor.audio_codec.device)\n",
        "        )\n",
        "\n",
        "class RepetitionPenaltyLogitsProcessorPatch(LogitsProcessor):\n",
        "    def __init__(self, penalty: float):\n",
        "        penalty_last_n = 64\n",
        "        print(\"üîÑ Using patched RepetitionPenaltyLogitsProcessor -> RepetitionPenaltyLogitsProcessorPatch | penalty_last_n: {penalty_last_n}\")\n",
        "        if penalty_last_n is not None:\n",
        "            if not isinstance(penalty_last_n, int) or penalty_last_n < 0:\n",
        "                raise ValueError(f\"`penalty_last_n` has to be a non-negative integer, but is {penalty_last_n}\")\n",
        "        if not isinstance(penalty, float) or penalty <= 0:\n",
        "            raise ValueError(f\"`penalty` has to be a positive float, but is {penalty}\")\n",
        "\n",
        "        self.penalty_last_n = penalty_last_n\n",
        "        self.penalty = penalty\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids (`torch.LongTensor`):\n",
        "                Indices of input sequence tokens in the vocabulary (shape `(batch_size, sequence_length)`).\n",
        "            scores (`torch.FloatTensor`):\n",
        "                Prediction scores of a language modeling head (shape `(batch_size, vocab_size)`).\n",
        "\n",
        "        Returns:\n",
        "            `torch.FloatTensor`: The modified prediction scores.\n",
        "        \"\"\"\n",
        "        # Check if penalties should be applied\n",
        "        if self.penalty_last_n == 0 or self.penalty == 1.0:\n",
        "            return scores\n",
        "\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        vocab_size = scores.shape[-1]\n",
        "\n",
        "        # Process each batch item independently\n",
        "        for b in range(batch_size):\n",
        "            # 1. Determine the penalty window\n",
        "            start_index = max(0, seq_len - self.penalty_last_n)\n",
        "            window_indices = input_ids[b, start_index:] # Shape: (window_len,)\n",
        "\n",
        "            if window_indices.numel() == 0: # Skip if window is empty\n",
        "                continue\n",
        "\n",
        "            # 2. Find unique tokens within the window\n",
        "            tokens_in_window = set(window_indices.tolist())\n",
        "\n",
        "            # 3. Apply repetition penalty to the scores for this batch item\n",
        "            for token_id in tokens_in_window:\n",
        "                if token_id >= vocab_size:\n",
        "                    continue\n",
        "\n",
        "                logit = scores[b, token_id]\n",
        "\n",
        "                if logit <= 0:\n",
        "                    logit *= self.penalty\n",
        "                else:\n",
        "                    logit /= self.penalty\n",
        "\n",
        "                # Update the score\n",
        "                scores[b, token_id] = logit\n",
        "\n",
        "        return scores\n",
        "\n",
        "generation_utils.RepetitionPenaltyLogitsProcessor = RepetitionPenaltyLogitsProcessorPatch\n",
        "AutoModelForCausalLM.generate = generation_utils.GenerationMixin.generate\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    formated_text = \"<|text_start|>\"+input_text+\"<|text_end|>\"\n",
        "    prompt = \"\\n\".join([\n",
        "        \"<|im_start|>\",\n",
        "        formated_text,\n",
        "        \"<|audio_start|><|global_features_start|>\",\n",
        "    ])\n",
        "    with torch.inference_mode():\n",
        "        with torch.amp.autocast('cuda',dtype=model.dtype):\n",
        "          model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "          print(\"Generating token sequence...\")\n",
        "          generated_ids = model.generate(\n",
        "              **model_inputs,\n",
        "              temperature=0.4,\n",
        "              top_k=40,\n",
        "              top_p=0.9,\n",
        "              repetition_penalty=1.1,\n",
        "              min_p=0.05,\n",
        "              max_new_tokens=2048, # Limit generation length\n",
        "          )\n",
        "          print(\"Token sequence generated.\")\n",
        "\n",
        "\n",
        "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
        "    audio = get_audio(generated_ids)\n",
        "    audio = audio.cpu()\n",
        "    from IPython.display import Audio, display\n",
        "    display(Audio(audio.squeeze(0), rate=24000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "5727fb16-0597-4c50-c5be-284878cffc5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1YjyrnPU0ST"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}